<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Paper Title - Project Page</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <style>
        body {
            font-family: Arial, sans-serif;
            background-color: #f8f9fa;
            text-align: center;
        }
        .container {
            max-width: 900px;
            margin-top: 20px;
        }
        .paper-title {
            font-size: 30px;
            font-weight: bold;
        }
        .author {
            font-size: 17px;
            color: #212529;
            font-weight: bold;
        }
        .affiliation {
            font-size: 16px;
            color: #6c757d;
        }
        .section-title {
            margin-top: 30px;
            font-weight: bold;
            border-bottom: 2px solid #007bff;
            display: inline-block;
            padding-bottom: 5px;
        }
        .image-container {
            width: 100%;
            max-width: 900px;
            margin: 10px auto;
        }
        .image-container img {
            width: 100%; /* ËÆ©ÂõæÁâáÂÆåÂÖ®ÈÄÇÈÖçÈ°µÈù¢ÂÆΩÂ∫¶ */
            height: auto; /* ‰øùÊåÅÁ∫µÊ®™ÊØîÔºåÈò≤Ê≠¢ÂèòÂΩ¢ */
            border-radius: 5px;
            display: block;
        }
        .video-container {
            position: relative;
            padding-bottom: 56.25%;
            height: 0;
            overflow: hidden;
            max-width: 900px;
            margin: 15px auto;
            background: #000;
        }
        .video-container iframe {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
        }
        .btn-container {
            margin-top: 15px;
        }
        .btn-custom {
            padding: 10px 16px;
            font-size: 16px;
            margin: 8px;
        }
        .abstract-content {
            text-align: left;
            margin: 10px auto;
            max-width: 900px;
        }
        .footer {
            margin-top: 20px;
            font-size: 14px;
            color: #6c757d;
        }
    </style>
</head>
<body>

<div class="container">
    <!-- Title -->
    <h1 class="paper-title">Filter Images First, Generate Instructions Later: <br> Pre-Instruction Data Selection for Visual Instruction Tuning</h1>
    <h2 class="cvpr-tag"><strong><span style="color: #d9534f;">CVPR 2025 (Highlight)</span></strong></h2>
    <p class="author">Bardia Safaei, Faizan Siddiqui, Jiacong Xu, Vishal M. Patel, Shao-Yuan Lo</p>
    <p class="affiliation">Johns Hopkins University, Honda Research Institute USA</p>

    <!-- Links -->
    <div class="btn-container">
        <a href="https://arxiv.org/abs/2503.07591" class="btn btn-danger btn-custom" target="_blank">üìÑ ArXiv</a>
        <a href="https://github.com/bardisafa/PreSel" class="btn btn-primary btn-custom" target="_blank">üîó GitHub</a>
        <a href="https://github.com/bardisafa/PreSel" class="btn btn-success btn-custom" target="_blank">üì• Dataset</a>
    </div>

    <!-- Abstract -->
    <h2 class="section-title">Abstract</h2>
    <p class="abstract-content">
        Visual instruction tuning (VIT) for large vision-language models (LVLMs) requires training on expansive datasets of 
        image-instruction pairs, which can be costly. Recent efforts in VIT data selection aim to select a small subset of 
        high-quality image-instruction pairs, reducing VIT runtime while maintaining performance comparable to full-scale training. 
        However, a major challenge often overlooked is that generating instructions from unlabeled images for VIT is highly expensive. 
        Most existing VIT datasets rely heavily on human annotations or paid services like the GPT API, which limits users with constrained
        resources from creating VIT datasets for custom applications. To address this, we introduce Pre-Instruction Data Selection (PreSel), 
        a more practical data selection paradigm that directly selects the most beneficial unlabeled images and generates instructions 
        only for the selected images. PreSel first estimates the relative importance of each vision task within VIT datasets to derive 
        task-wise sampling budgets. It then clusters image features within each task, selecting the most representative images with the budget. 
        This approach reduces computational overhead for both instruction generation during VIT data formation and LVLM fine-tuning. 
        By generating instructions for only 15% of the images, PreSel achieves performance comparable to full-data VIT on the LLaVA-1.5 and Vision-Flan datasets.
    </p>

    <!-- Method Figure -->
    <h2 class="section-title">Motivation</h2>
    <div class="image-container">
        <img src="demos/motivation.png" alt="Your Figure" style="max-width: 50%; height: auto; display: block; margin: 0 auto;">


    </div>
    
    <!-- Method Figure -->
    <h2 class="section-title"><i>PreSel</i> Framework</h2>
    <div class="image-container">
        <img src="demos/main_framework.png" alt="Method Illustration">
    </div>

    <!-- Results Section -->
    <h2 class="section-title">Main Results</h2>
    <div class="image-container">
        <figure>
            <img src="demos/results_llava.png" alt="Results on LLaVA-1.5">
            <figcaption>Table 1: Performance comparison of PreSel on LLaVA-1.5. Our method only requires instruction generation for the selected images (15%). </figcaption>
        </figure>
    </div>

    <div class="image-container">
        <figure>
            <img src="demos/results_vf.png" alt="Results on Vision-Flan">
            <figcaption>Table 2: Performance comparison of PreSel on Vision-Flan. Our method only requires instruction generation for the selected images (15%).</figcaption>
        </figure>
    </div>

    <div class="image-container">
        <figure>
            <img src="demos/diff_archs.png" alt="Results on Vision-Flan">
            <figcaption>Table 3: Results for LLaVA-Vicuna-13B and LLaVA-Llama-8B models.</figcaption>
        </figure>
    </div>


    <!-- Cost Section -->
    <h2 class="section-title">Cost Analysis</h2>
    <div class="image-container">
        <figure>
            <img src="demos/cost_analysis.png" alt="Results on LLaVA-1.5">
            <figcaption>Table 4: Comparison of VIT costs for PreSel, other VIT data selection methods, and full-scale LVLM fine-tuning.</figcaption>
        </figure>
    </div>

    
    <!-- Dataset -->
    <h2 class="section-title">Dataset</h2>
    <p class="dataset-content">
        The data selected by PreSel will be released soon!
    </p>

    <!-- Citation Section -->
    
    <h2 class="section-title">Citation</h2>
    <div class="citation-container">
        <p>If you find our work useful, please cite:</p>
        <pre>
@article{safaei2025filter,
    title={Filter Images First, Generate Instructions Later: Pre-Instruction Data Selection for Visual Instruction Tuning},
    author={Safaei, Bardia and Siddiqui, Faizan and Xu, Jiacong and Patel, Vishal M and Lo, Shao-Yuan},
    journal={arXiv preprint arXiv:2503.07591},
    year={2025}
  }
        </pre>
    </div>
    
    <!-- CSS for Citation Styling -->
    
    <style>
        .citation-container {
            background-color: #f8f9fa;
            border-left: 4px solid #007bff;
            padding: 10px;
            margin: 20px auto;
            max-width: 900px;
            text-align: left;
            font-family: monospace;
        }
        pre {
            white-space: pre-wrap;
            word-wrap: break-word;
            background-color: #ffffff;
            padding: 10px;
            border-radius: 5px;
            border: 1px solid #ddd;
        }
    </style> 
    
    <p class="footer">¬© 2025 Bardia Safaei | Last updated: March 2025</p>
</div>

<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>
